{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple XGBoost Regressor with TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF IDF applied to all text present in data, that means I have concatenated all string columns then applied Tf Idf vectorizer. I believe that working on each column seperately and performing some Hyper Parameter Tuning (for ex with grid-search) could improve much more the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import xgboost as xgb\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_points(text):\n",
    "    kein_punkt=\"\".join([c for c in text if c not in string.punctuation])\n",
    "    return kein_punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "def tokenize(text):\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    text=tokenizer.tokenize(text.lower())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)\n",
    "#use of dict to reduce processing time since a search of a word in a dict is o(1)\n",
    "def remove_stopWords(text):\n",
    "    words = [w for w in text if w not in stopwords_dict]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "def word_stemmer(text):\n",
    "    stem_text=[stemmer.stem(w) for w in text]\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing function\n",
    "def preprocessing(text,rm_points=False,token=False,rm_stop=False,stem=False):\n",
    "    if rm_points:\n",
    "        text=remove_points(text)\n",
    "    if token:\n",
    "        text=tokenize(text)\n",
    "    if rm_stop:\n",
    "        text=remove_stopWords(text)\n",
    "    if stem:\n",
    "        text=word_stemmer(text)\n",
    "    text=\" \".join([w for w in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'description', 'designation', 'points', 'price', 'province',\n",
       "       'region_1', 'region_2', 'taster_name', 'taster_twitter_handle', 'title',\n",
       "       'variety', 'winery', 'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"all_text\"]=data['country'].astype(str)+data['description'].astype(str)+data['designation'].astype(str)+data['province'].astype(str)+data['region_1'].astype(str)+data['region_2'].astype(str)+data['taster_name'].astype(str)+data['taster_twitter_handle'].astype(str)+data[ 'title'].astype(str)+data['variety'].astype(str)+data['winery'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"all_text\"]=test['country'].astype(str)+test['description'].astype(str)+test['designation'].astype(str)+test['province'].astype(str)+test['region_1'].astype(str)+test['region_2'].astype(str)+test['taster_name'].astype(str)+test['taster_twitter_handle'].astype(str)+test[ 'title'].astype(str)+test['variety'].astype(str)+test['winery'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text=pd.concat([data[\"all_text\"],test[\"all_text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_text=[]\n",
    "test_all_text=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,raw in data.iterrows():\n",
    "    train_all_text.append(preprocessing(raw[\"all_text\"],True,True,True,False))\n",
    "for idx,raw in test.iterrows():\n",
    "    test_all_text.append(preprocessing(raw[\"all_text\"],True,True,True,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    norm='l2',\n",
    "    min_df=0,\n",
    "    smooth_idf=False,\n",
    "    max_features=15000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(data[\"all_text\"])\n",
    "test_word_features = word_vectorizer.transform(test[\"all_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "stacked=sparse.hstack((train_word_features,np.array(data[\"points\"])[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stacked=sparse.hstack((test_word_features,np.array(test[\"points\"])[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dmatrix=xgb.DMatrix(data=test_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=stacked,label=data[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:44.0022+0.362275\ttest-rmse:45.0368+0.700396\n",
      "[2]\ttrain-rmse:34.2939+0.321524\ttest-rmse:37.2147+0.614473\n",
      "[4]\ttrain-rmse:28.6768+0.333574\ttest-rmse:33.1803+0.625382\n",
      "[6]\ttrain-rmse:25.4342+0.2263\ttest-rmse:31.0721+0.749956\n",
      "[8]\ttrain-rmse:23.5492+0.173856\ttest-rmse:30.0246+0.773033\n",
      "[10]\ttrain-rmse:22.2519+0.153842\ttest-rmse:29.3894+0.861841\n",
      "[12]\ttrain-rmse:21.3689+0.10173\ttest-rmse:28.9026+0.890044\n",
      "[14]\ttrain-rmse:20.7298+0.112303\ttest-rmse:28.6061+0.922361\n",
      "[16]\ttrain-rmse:20.3117+0.118437\ttest-rmse:28.3922+0.933194\n",
      "[18]\ttrain-rmse:19.8793+0.104567\ttest-rmse:28.2003+0.911104\n",
      "[20]\ttrain-rmse:19.5409+0.0622652\ttest-rmse:28.052+0.902495\n",
      "[22]\ttrain-rmse:19.2648+0.0459628\ttest-rmse:27.9257+0.900756\n",
      "[24]\ttrain-rmse:19.0118+0.0676671\ttest-rmse:27.8465+0.890411\n",
      "[26]\ttrain-rmse:18.7304+0.0741378\ttest-rmse:27.7643+0.887743\n",
      "[28]\ttrain-rmse:18.4898+0.0752081\ttest-rmse:27.6664+0.873278\n",
      "[30]\ttrain-rmse:18.2086+0.0308173\ttest-rmse:27.5649+0.884311\n",
      "[32]\ttrain-rmse:18.0354+0.0158905\ttest-rmse:27.5083+0.892076\n",
      "[34]\ttrain-rmse:17.8545+0.0222553\ttest-rmse:27.4286+0.897064\n",
      "[36]\ttrain-rmse:17.6761+0.0450397\ttest-rmse:27.3572+0.903101\n",
      "[38]\ttrain-rmse:17.5149+0.0420658\ttest-rmse:27.3049+0.896517\n",
      "[40]\ttrain-rmse:17.3458+0.0229101\ttest-rmse:27.2464+0.903418\n",
      "[42]\ttrain-rmse:17.1879+0.020404\ttest-rmse:27.2028+0.899839\n",
      "[44]\ttrain-rmse:17.0384+0.0344388\ttest-rmse:27.152+0.905407\n",
      "[46]\ttrain-rmse:16.886+0.038736\ttest-rmse:27.121+0.898963\n",
      "[48]\ttrain-rmse:16.7395+0.0480391\ttest-rmse:27.0697+0.89429\n",
      "[49]\ttrain-rmse:16.669+0.0445899\ttest-rmse:27.0514+0.892357\n"
     ]
    }
   ],
   "source": [
    "params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 1,'learning_rate': 0.2,\n",
    "                'max_depth': 10, 'alpha': 3,'booster':'dart'}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123,verbose_eval=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49    27.051424\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=xg_reg.predict(test_dmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame({'id':test['id'],'price':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>101.782936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>41.472393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29.744003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>31.308027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20.481169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       price\n",
       "0   0  101.782936\n",
       "1   1   41.472393\n",
       "2   2   29.744003\n",
       "3   3   31.308027\n",
       "4   4   20.481169"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.782936"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result in LB 23.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
